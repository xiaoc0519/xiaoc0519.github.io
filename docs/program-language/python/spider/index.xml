<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaoc0519</title>
    <link>https://xiaoc0519.github.io/docs/program-language/python/spider/</link>
    <description>Recent content on Xiaoc0519</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 16 Sep 2023 15:01:01 +0800</lastBuildDate><atom:link href="https://xiaoc0519.github.io/docs/program-language/python/spider/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Charles</title>
      <link>https://xiaoc0519.github.io/docs/program-language/python/spider/charles/</link>
      <pubDate>Sat, 16 Sep 2023 15:01:01 +0800</pubDate>
      
      <guid>https://xiaoc0519.github.io/docs/program-language/python/spider/charles/</guid>
      <description>CHARLES#下载#Charles LICENSE // Charles Proxy License Registered Name: https://zhile.io License Key: 48891cf209c6d32bf4 配置#PC端#1 Proxy Setting - HTTP Proxy check all
2 Proxy - SSL Proxying Setting - Enable SSL Proxying
add Host * Port *
3 HELP - SSL Proxying - Install Charles Root Certificate on a Mobile...
手机端#和电脑处于同一局域网下，并设置代理为电脑ip
打开 chls.pro/ssl 下载安装证书</description>
    </item>
    
    <item>
      <title>Css_selector</title>
      <link>https://xiaoc0519.github.io/docs/program-language/python/spider/css_selector/</link>
      <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://xiaoc0519.github.io/docs/program-language/python/spider/css_selector/</guid>
      <description>CSS_LELECTOR#span&amp;gt;span[name:value] span:: .class1.class2 # 一个标签多个class span.classname # span标签特定classname spanname[name=value] # 属性值定位 属性名=属性值 [id=&amp;#34;IamID&amp;#34;][name=&amp;#34;first&amp;#34;] # 多属性定位 .form span # class=from 下所有子孙 span 标签 .form&amp;gt;span # class=from 下所有下一级 span 标签 .Dream + br # 只能向后一个选择同级标签 .Dream ~ br # 向后选择同级标签 :nth-child(n) # 匹配属于其父元素下的第n个子元素 option:nth-child(3) # option标签的父标签的第三个子元素 :nth-last-child(n) # 匹配属于其父元素下的倒数第n个子元素 :first-child # 匹配属于其父元素下的第1个子元素 :last-child # 匹配属于其父元素下的最后1个子元素 # class值一样 id随机生成 # :contains() 访问页面上DOM tree之外的信息 &amp;#39;.xbutton:contains(&amp;#34;OK&amp;#34;)&amp;#39; # 定位class为 xbutton 的一个显示OK的Button div[class=u-input box]&amp;gt;input:nth-child(2) </description>
    </item>
    
    <item>
      <title>Execjs</title>
      <link>https://xiaoc0519.github.io/docs/program-language/python/spider/execjs/</link>
      <pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://xiaoc0519.github.io/docs/program-language/python/spider/execjs/</guid>
      <description>EXECJS#pip install pyexecjs2 </description>
    </item>
    
    <item>
      <title>Xpath</title>
      <link>https://xiaoc0519.github.io/docs/program-language/python/spider/xpath/</link>
      <pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://xiaoc0519.github.io/docs/program-language/python/spider/xpath/</guid>
      <description>XPATH#from lxml import etree text = &amp;#39;html reponse text&amp;#39; html = etree.HTML(text) result:list = html.xpath(&amp;#39;//div[@id=&amp;#34;s-top-left&amp;#34;]//a[text()=&amp;#34;学术&amp;#34;]&amp;#39;) # div id 为 s-top-left 的下面 所有的 a 标签的 文本 为 学术 # nodename 选取此节点的所有子节点 bookstore # 选取 bookstore 元素的所有子节点 # / 从根节点选取（取子节点） # 假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ /bookstore # 选取根元素 bookstore bookstore/book # 选取属于 bookstore 的子元素的所有 book 元素 # // 从匹配选择的当前节点选择文档中的节点 //book # 选取所有 book 子元素，而不管它们在文档中的位置 bookstore//book # 选择属于 bookstore 元素的所有 book 元素 # 取值 .</description>
    </item>
    
    <item>
      <title>Bs4</title>
      <link>https://xiaoc0519.github.io/docs/program-language/python/spider/bs4/</link>
      <pubDate>Tue, 23 May 2023 15:01:01 +0800</pubDate>
      
      <guid>https://xiaoc0519.github.io/docs/program-language/python/spider/bs4/</guid>
      <description>BeautifulSoup LXML## pip install bs4 # pip install lxml from bs4 import BeautifulSoup req=requests.get(url,headers=headers) html=req.text soup = BeautifulSoup(html, &amp;#39;lxml&amp;#39;) # 创建beautifulsoup解析对象 soup.prettify() # 格式化输出 html / xml 文档 Beautiful Soup 是一个HTML/XML 的解析器，主要用于解析和提取 HTML/XML 数据。
Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,
所有对象可以归纳为4种: Tag , NavigableString , BeautifulSoup , Comment .
取值## Tag有很多方法和属性，tag中最重要的属性: `name` 和 `attributes`。 soup=BeautifulSoup(html,&amp;#39;lxml&amp;#39;) soup.h1 # 网页匹配的第一个标签 soup.div.attrs # 获取标签div所有属性,返回字典 soup.ol[&amp;#39;class&amp;#39;] # 获取标签ol属性为class的值 # NavigableString ## ** 标签内非属性字符串,格式：soup.&amp;lt;tag&amp;gt;.string ## ** NavigableString 可以跨越多个层次 soup.</description>
    </item>
    
    <item>
      <title>Re</title>
      <link>https://xiaoc0519.github.io/docs/program-language/python/spider/re/</link>
      <pubDate>Tue, 23 May 2023 08:56:23 +0800</pubDate>
      
      <guid>https://xiaoc0519.github.io/docs/program-language/python/spider/re/</guid>
      <description>RE#import re result = re.match(pattern, string, re.I) result.group() # ab 返回整体结果 result.group(1) # b 返回第一个()匹配部分 Flags#re.I # 忽略大小写 re.L # 表示特殊字符集 \w, \W, \b, \B, \s, \S 依赖于当前环境 re.M # 多行模式 re.S # 即为 . 并且包括换行符在内的任意字符（. 不包括换行符） re.U # 表示特殊字符集 \w, \W, \b, \B, \d, \D, \s, \S 依赖于 Unicode 字符属性数据库 re.X # 为了增加可读性，忽略空格和 # 后面的注释 Pattern#单字符#. # 匹配任意1个字符（除了\n） [ ] # 匹配[ ]中列举的字符 \d # 数字，即0-9 可以写在字符集[.</description>
    </item>
    
    <item>
      <title>Scrapy</title>
      <link>https://xiaoc0519.github.io/docs/program-language/python/spider/scrapy/</link>
      <pubDate>Mon, 22 May 2023 22:44:05 +0800</pubDate>
      
      <guid>https://xiaoc0519.github.io/docs/program-language/python/spider/scrapy/</guid>
      <description>SCRAPY#WORKFLOW#scrapy startproject projectname # 创建项目文件 cd projectname scrapy genspider spidername spiderwebsite # 创建项目爬虫网页 # scrapy genspider douban movie.douban.com scrapy crawl spidername [-o name.csv] # 运行爬虫 指定输出格式 FILES#projectname/ scrapy.cfg # 项目的配置文件。 projectname/ # 项目的Python模块，将会从这里引用代码 __init__.py items.py # 项目的目标文件 pipelines.py # 项目的管道文件 settings.py # 项目的设置文件 spiders/ # 存储爬虫代码目录 __init__.py spidername.py ... SETTINGS.py## Scrapy settings for mySpider project ... BOT_NAME = &amp;#39;mySpider&amp;#39; # scrapy项目名 SPIDER_MODULES = [&amp;#39;mySpider.</description>
    </item>
    
    <item>
      <title>Httpx</title>
      <link>https://xiaoc0519.github.io/docs/program-language/python/spider/httpx/</link>
      <pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://xiaoc0519.github.io/docs/program-language/python/spider/httpx/</guid>
      <description>HTTPX#3.6+ pip install httpx,httpx[http2]
import httpx res = httpx.get(&amp;#39;url&amp;#39;,timeout=10.0) # 超时默认 5s None 可以关闭超时 HTTP/2#httpx.Client() 类似于 requests.Session()
client = httpx.Client(http2=True, verify=False) response = client.get(url, headers) response.text SPIDER#client = httpx.Client() #类似requests.Session() try: do somting finally: client.close() #关闭连接池 with httpx.Client() as client: r = client.get(url, headers=headers) # Client 和 get 里面都可以添加 headers,最后这两个地方的 headers 可以合并到请求里 headers = {&amp;#39;X-Auth&amp;#39;: &amp;#39;from-client&amp;#39;} params = {&amp;#39;client_id&amp;#39;: &amp;#39;client1&amp;#39;} with httpx.Client(headers=headers, params=params) as client: headers = {&amp;#39;X-Custom&amp;#39;: &amp;#39;from-request&amp;#39;} params = {&amp;#39;request_id&amp;#39;: &amp;#39;request1&amp;#39;} r = client.</description>
    </item>
    
    <item>
      <title>Requests</title>
      <link>https://xiaoc0519.github.io/docs/program-language/python/spider/requests/</link>
      <pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://xiaoc0519.github.io/docs/program-language/python/spider/requests/</guid>
      <description>REQUESTS#import requests,json res = requests.get(url,headers,params,json,verify=false,cookies,timeout) # params get请求传参 a=1 # data 非get请求提交数据, res.body的格式为 a=1&amp;amp;b=2 # json 提交数据，res.body的格式为 {“a”: 1, “b”: 2} res = res.text # 文本内容 res = res.content # 2进制内容 json.loads(json) # json数据 Requests#requests.get() requests.post() requests.head() # 获取网页头的信息 requests.put() # 提交put请求 requests.delete() # 向HTML页码提交删除请求 requests.patch() # 向HTML网页提交局部修改请求 requests.get( url,headers,cookies,timeout, json, # 提交数据，res.body的格式为 {“a”: 1, “b”: 2} params, # get请求传参 a=1 data, # 非get请求提交数据, res.body的格式为 a=1&amp;amp;b=2 auth, # 元组，支持HTTP认证功能 files, # 字典类型，传输文件 proxies, # 字典类型，设定访问代理服务器，可以增加登录认证 allow_redirects, # True/False，默认为True，重定向开关 stream, # True/False， 默认为True，获取内容立刻下载开关 verify, # True/False，默认为True，认证SSL证书开关 cert # 本地SSL证书路径 ) Response## cookie 获取处理cookie cookie = res.</description>
    </item>
    
    <item>
      <title>Selenium</title>
      <link>https://xiaoc0519.github.io/docs/program-language/python/spider/selenium/</link>
      <pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://xiaoc0519.github.io/docs/program-language/python/spider/selenium/</guid>
      <description>SELENIUM 4#驱动下载: Firefox Chrome Edge
启动#from selenium import webdriver from selenium.webdriver.chrome.service import Service as ChromeService service = ChromeService(executable_path=CHROMEDRIVER_PATH) driver = webdriver.Chrome(service=service) 参数#option = webdriver.Options() options.add_argument(&amp;#39;user-agent=&amp;#34;value&amp;#34;&amp;#39;) # 添加UA options.add_argument(&amp;#39;window-size=1920x3000&amp;#39;) # 指定浏览器分辨率 options.add_argument(&amp;#39;--disable-gpu&amp;#39;) # 谷歌文档提到需要加上这个属性来规避bug options.add_argument(&amp;#39;--hide-scrollbars&amp;#39;) # 隐藏滚动条, 应对一些特殊页面 options.add_argument(&amp;#39;blink-settings=imagesEnabled=false&amp;#39;) # 不加载图片, 提升速度 options.add_argument(&amp;#39;--headless&amp;#39;) # 不提供可视化页面. linux系统不支持可视化不加这条会启动失败 options.add_argument(&amp;#39;--no-sandbox&amp;#39;) # 以最高权限运行 options.binary_location = r&amp;#34;path&amp;#34; # 手动指定使用的浏览器位置 options.add_extension(&amp;#39;crx&amp;#39;) # 添加 CRX 插件 options.add_argument(&amp;#34;--disable-javascript&amp;#34;) # 禁用JavaScript options.add_argument(&amp;#34;--proxy-server=http://XXXXX.com:80&amp;#34;) # 添加代理 # 设置开发者模式启动，该模式下webdriver属性为正常值 options.</description>
    </item>
    
  </channel>
</rss>
