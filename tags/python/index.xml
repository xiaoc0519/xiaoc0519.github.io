<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on Xiaoc0519</title>
    <link>https://xiaoc0519.github.io/tags/python/</link>
    <description>Recent content in python on Xiaoc0519</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 May 2023 15:48:25 +0800</lastBuildDate><atom:link href="https://xiaoc0519.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>81zw async</title>
      <link>https://xiaoc0519.github.io/posts/81zw-async/</link>
      <pubDate>Tue, 30 May 2023 15:48:25 +0800</pubDate>
      
      <guid>https://xiaoc0519.github.io/posts/81zw-async/</guid>
      <description>import asyncio import time import aiohttp from bs4 import BeautifulSoup as bs async def main(): async with aiohttp.ClientSession() as session: b_url = &amp;#39;http://81zw.com.cn/book/24/&amp;#39; headers = { &amp;#39;User-Agent&amp;#39;:&amp;#39;Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/114.0&amp;#39;, &amp;#39;Cookie&amp;#39;:&amp;#39;ck&amp;#39; } res = await session.get(b_url,headers=headers) html = await res.text() soup = bs(html,&amp;#39;lxml&amp;#39;) links = soup.select(&amp;#39;.listmain a&amp;#39;) bookname = soup.select(&amp;#39;#info&amp;gt;h1&amp;#39;)[0].text tasks = [link_info(session,link[&amp;#39;href&amp;#39;]) for link in links] content = await asyncio.gather(*tasks) with open(bookname+&amp;#39;.txt&amp;#39;,&amp;#39;w&amp;#39;,encoding=&amp;#39;utf-8&amp;#39;) as f: for item in content: f.</description>
    </item>
    
    <item>
      <title>Dingdian001 async</title>
      <link>https://xiaoc0519.github.io/posts/dingdian001-async/</link>
      <pubDate>Tue, 30 May 2023 15:48:25 +0800</pubDate>
      
      <guid>https://xiaoc0519.github.io/posts/dingdian001-async/</guid>
      <description>import asyncio import aiohttp from bs4 import BeautifulSoup as bs from loguru import logger URL = &amp;#39;https://www.dingdian001.com/xiaoshuo_3405/&amp;#39; HEADERS = {&amp;#39;User-Agent&amp;#39;:&amp;#39;Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/117.0&amp;#39;} async def main(): async with aiohttp.ClientSession() as session: res = await session.get(url = URL,headers=HEADERS) html = await res.text() soup = bs(html,&amp;#39;lxml&amp;#39;) bookname = soup.select(&amp;#39;#info h1&amp;#39;)[0].text logger.success(f&amp;#39;{bookname=}&amp;#39;) body = soup.select(&amp;#39;#list dl&amp;#39;)[0] zone = body.select(&amp;#39;dt&amp;#39;) if len(zone) == 1: pass if len(zone) == 2: chapter = body.</description>
    </item>
    
    <item>
      <title>Taobao</title>
      <link>https://xiaoc0519.github.io/posts/taobao/</link>
      <pubDate>Tue, 30 May 2023 15:48:25 +0800</pubDate>
      
      <guid>https://xiaoc0519.github.io/posts/taobao/</guid>
      <description>website taobao spider
scrapy startproject projectname cd projectname scrapy genspider spidername spiderwebsite pip install -r requirements.txt 创建浏览器#from selenium import webdriver from selenium.webdriver.chrome.service import Service as ChromeService def creat_chrome_driver(*,headless=False): option = webdriver.Options() if headless: options.add_argument(&amp;#39;--headless&amp;#39;) options.add_experimental_option(&amp;#39;excludeSwitches&amp;#39;,[&amp;#39;enable-automation&amp;#39;]) options.add_experimental_option(&amp;#39;useAutomationExtension&amp;#39;,False) browser = webdriver.Chrome(options=options) browser.execute_cdp_cmd( &amp;#39;Page.addScriptToEvaluateOnNewDocument&amp;#39;, {&amp;#39;source&amp;#39;:&amp;#39;Object.defineProperty(navigator,&amp;#34;webdriver&amp;#34;,{get: () =&amp;gt; undefined})&amp;#39;} ) return browser def add_cookies(browser,cookie_file): with open(cookie_file,&amp;#39;r&amp;#39;) as f: cookie_list = json.load(f) for cookie_dict in cookies_list: if cookie_dict[&amp;#39;secure&amp;#39;]: browser.add_cookie(cookie_dict) 从浏览器获取cookie#browser = creat_chrome_driver() browser.</description>
    </item>
    
  </channel>
</rss>
